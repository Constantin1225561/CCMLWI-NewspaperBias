
%  (modified by George Kachergis from Michael Weeks' example_final_report2.tex)
\documentclass[final]{ieee}
\usepackage{graphics}
\usepackage{tabularx}
\journal{CCMLWI}

\title[Cognitive Computational Modeling of Language and Web Interaction]{Bias in Newspaper Headlines}
\author[Br\^{i}ncoveanu \& van Niedek]{%
   Constantin Br\^{i}ncoveanu \\
   Timo van Niedek
    \authorinfo{%
     Cognitive Computational Modeling of Language and Web Interaction,\\
      SOW-MKI61-2016-SEM2-V, 13th July 2017, Dr. G.E. Kachergis. \\
      email: \mbox{c.brincoveanu@student.ru.nl},\mbox{timo.niedek@student.ru.nl}} 
}

\begin{document}

\maketitle

\begin{abstract}
The aim of this project is to detect bias in newspaper headlines using machine learning. A dataset containing world news headlines was obtained using the Reddit API. Bias detection was achieved using several approaches. Firstly, Sentiment Analysis was applied on the headlines. Mean sentiment values can provide information about possible biases. Topic Detection was performed to categorize the headlines, and then Sentiment Analysis was performed on those topic classes, as well as Flavor Analysis. Statistical tests yielded significant biases for a few domains. Lastly, we employed domain classification to gain a different perspective on the biases.
%The abstract should be no more than 150 words.
%It is a short summary of the paper. If you had to
%re-state what your paper says in 150 words or less, what would you
%say? By the way, I recommend writing the abstract LAST, since it is
 %    easier this way.
%      For a conference paper, most people will read the abstract to see
 %     if they find it interesting enough to read the whole paper. This
  %    makes a lot of sense if you go to a conference in a topic that
  %    interests you, but find that there are 100+ other papers.
\end{abstract}

\section{Introduction}\label{sec:intro}

Although newspapers tend to claim neutrality, it is almost impossible to find a source that does not contain any stereotypes or biases in some form. Biases in news headlines pose a danger because they can propagate stereotypes to the general public. The uninterested or time-pressed reader will skim the headlines of newspaper articles or online news collections, often missing important nuances. It is therefore important to be able to detect biased language not only in the body of a news article, but also in the headline by itself. An automatic bias detection system could help guide readers, and expose the hidden biases that are present in news headlines.

Detecting biases manually requires an enormous amount of effort to code news headlines and apply theoretical bias frameworks. It is infeasible to apply this process to the vast amounts of news stories that are released every day. An alternative approach that has gained interest recently is to detect biases automatically using machine learning techniques. These systems employ natural language processing techniques to detect one of the many ways in which bias can be present in texts. One such method is sentiment analysis, in which the polarity of the text is determined, either positive, negative or neutral. Sentiment analysis is typically applied to highly subjective texts for which the polarity is very clear and has high variance across difference texts. The difficulty with using sentiment analysis on news headlines to detect bias is that the polarity scores lie closely together, and therefore do not clearly indicate a bias on its own. News outlets report negative events such as war, death or protests more often than positive events, which makes it difficult to compare polarity scores. Additionally, bias detection using polarity as a measure is not enough since negative scores appear often, even unbiased reports. We therefore propose a technique that investigates the difference between the headline sentiment and the mean of the polarity scores for a specific topic. 

Another method of investigating news bias is to measure the amount of flavor words used by a specific news source. These flavor words are adjectives, adverbs, comparatives and superlatives. The motivation behind this method is that these words can carry bias, and a truly objective report would need less of them. Our method compares the amount of flavor in a headline with the mean of the flavor across one topic to identify which news sources use significantly more flavor as an indicator of bias. 

Our third and final approach is to classify news headlines into their respective sources, which will indicate how distinct a news source is. An unbiased news outlet would be very difficult to classify, whereas a news outlet that consistently uses biased language can easily be classified.

Our methods can be applied in real-world, on-line settings as a tool to make readers aware of the presence of bias. Creating awareness is an important first step in preventing the reinforcement and spreading of unwanted stereotypes. Our proposed methods are applicable to topics and news sources of any kind.

The rest of this paper is structured as follows. In Section \ref{sec:background} we provide an overview of related work, as well as giving the definition of bias that we used to define our work. Section \ref{sec:project} describes the technical details of our work, where Section \ref{sec:project}.\ref{sec:method} describes the methods that we used to detect bias and Section \ref{sec:project}.\ref{sec:evaluation} details how we validated the performance of our techniques. We provide the results in Section \ref{sec:results} and conclude with Section \ref{sec:conclusions}. 

%     The introduction should get the reader interested, explain your
 %    motivations, and provide a quick guide to the rest of the paper.
     
 %     Why is your topic important? Convince us!
 %     Where is it used? What are the applications.
  %    What will you talk about, and what did you do?

  %   Include an overview of the rest of your paper (e.g., section 2
     %       covers...section 3 presents...).
            
            
\section{Background}\label{sec:background}

In cognitive science and sociology, biases in news reports have been studied since well before automatic detection was possible. The first crucial step for research into this topic is to find a clear, measurable definition of bias. One such definition with respect to news reports is \begin{quotation}
``[\dots] the inappropriate intrusion of subjective opinion into an otherwise factual account''.\cite{STOCKING89}
\end{quotation}
In general, cognitive biases are often defined as a deviation from a norm. We therefore combine this definition with the one above, to define a biased headline as a headline that deviates significantly in some dimension from the average value.
  
There is other research in which bias detection is performed on news texts. Two forms of linguistic biases, namely framing and epistemological biases are detected using heuristic methods in \cite{RECASENS13}. Political biases can be detected using ``memes'', phrases from politicians that are quoted in online texts \cite{GUPTA09}. Instead, we use more general approaches to detect bias, that can be applied to any topic.

In other work, bias is detected in a supervised manner using a SVM that can classify the headlines of a small dataset into two classes (biased and unbiased) with an accuracy of $93.66\%$ \cite{CHU}. In contrast with this work, we use unsupervised methods on a larger data set.

Doumit et al. \cite{DOUMIT11} use a combination of latent Dirichlet allocation to detect the topic signatures similar to our work, and use them to create a model in which biases are characterized as variants on the topic signature as detected by natural language processing methods. The difference between their work and ours, is that we directly determine the bias per news source as a quantitative measure, so that bias scores can easily be compared between different sources.

To the authors' knowledge, there is no prior work that uses a classifier for determining the news source from a headline as an indicator of bias.

% Include any relevant and specific info,
 %e.g. hardware statistics, equipment used.
% There is {\bf always} some background to cover.
%            
%           Describe what other people had to say on this topic(s)
%            (be sure to cite your references, and quote as appropriate).
%           Describe what other people did on this topic (or related topics).
%           In the references section, cite the papers/books that you used.
%Talk about problems and shortcomings of their work.
%Talk about how your work is different and better.
%
%Here are some citation examples. 
%This is a book about VLSI \cite{Weste93}.
%Also, the references contain a good conference paper \cite{LiY88},
%and a good journal article \cite{BiS92}.
%Anything you found useful.
%Include textbooks from class if you want.


\section{Project}\label{sec:project}

Our project consists of several approaches which let us detect biases and analyze newspapers from different perspectives. In this section, we are going to provide information about the data set we used, as well as describe our approaches, ranging from general sentiment analysis, topic detection and sentiment and flavor analysis for the specific topics, to domain classification.

\subsection{Data}\label{sec:data}

Firstly, we looked at the Kaggle News Aggregator Dataset\cite{Lichman13}. The advantage of this dataset is that it contains over 400k news stories from many different domains. Often, the same event exists in the dataset multiple times, covered by different news sources.
This dataset did not fulfill our requirements, because it only covered a quite small timespan in 2014 and it had a limited number of content categories, and no politic content.

Therefore, we started looking for alternative datasets, which led us to the discovery of the Reddit API. We selected newspaper headlines from "r/worldnews", which exhibits roughly 90k headlines per year. Each headline was obtained with a corresponding timestamp and the domain on which the news story was published. One drawback of our dataset is the fact that each event was usually featured only once in our dataset. If the same event would exist in the dataset several times, covered by different domains, a direct comparison between those domains would have been much easier. We scraped headlines starting from 2014, but for our analysis we mostly focused on the timespan between 2016 and 2017, since the full dataset would make Topic Detection too computationally expensive. Furthermore, the number of topics would probably increase, and some topics would only occur during a short timespan.

Since many domains in the dataset only occurred a few times, and the number of headlines per domain would be further reduced because of the Topic Detection, we selected the big domains to make sure that at least a few hundred headlines per domain were available.

\subsection{Method}\label{sec:method}

%Describe your approach to the problem.
%Describe what you did.
%            What you already had (and where it came from).
%            What you added/changed.
%            For parts, include close-up drawings (e.g. Magic screenshots).

We employed several methods to detect biases. As a first glance at a possible bias, we used Sentiment Analysis. Later, we decided to refine our results by utilizing Topic Detection, Flavor Analysis, as well as a domain classifier.

\subsubsection{General Sentiment Analysis}\label{sec:general sentiment}

One of our first results was a general Sentiment Analysis. We use the VADER sentiment analysis framework \cite{VAD14} since it is has been shown to achieve good performance on short social media text. VADER is able to detect the polarity of complicated sentences with human-like accuracy.

We aggregated the results by averaging the sentiments of the headlines for each domain. This revealed some interesting differences between the domains. It became clear that some domains had very negative average sentiments, because they focused on negative topics such as war and general world news, whereas others have relatively positive average sentiments, because they focused on economics or science news.

\subsubsection{Topic Detection}\label{sec:topic detection}

As a more fine-grained measure of bias, we use sentiment analysis per topic. To extract the topics from our data set, we use the biterm topic model (BTM) \cite{BTM13}, which is especially suitable for short texts. BTM uses the word co-occurrence patterns (biterms) to create a model over the entire corpus, thus solving the problem of sparse word co-occurrences within one document. The model parameters are estimated using Gibbs sampling. We create a vocabulary of the top 5000 most occurring words in the corpus, and run the Gibbs sampling algorithm to model $T = 20$ topics. For the Dirichlet priors, we use $\alpha = 50/T = 2.5$ as recommended by \cite{FST13} and $\beta = 0.01$ in order to make the topics contain a small number of highly distinct words. Due to limitations in computational power and the large size of the data set, we were limited to 250 iterations of Gibbs sampling, which took approximately nine hours to complete. We use a modified version of the BTM implementation found at \cite{BTM15}.

\subsubsection{Topic Sentiment Analysis}\label{sec:topic sent}

We determine the topic for all the headlines in the 2017 Reddit data set using the maximum topic probability. We extract the headlines for the selected topics in Table \ref{tab:topics} and compute the polarity score for every headline using VADER \cite{VAD14}.

For every topic, we take the mean over the polarity scores of all headlines within the topic, which we call the \textit{mean topic sentiment}. We then calculate the mean over the polarity scores within one topic for every news source separately. The difference between the mean sentiment per news source and the mean topic sentiment is an indicator of bias, using the definition of bias given in Section \ref{sec:background}.

\subsubsection{Flavor Analysis}\label{sec:flavor}

As an alternative metric, we introduce the concept of flavor. The amount of flavor in a headline is determined by the amount of words that can potentially carry subjectivity, namely adjectives, adverbs, superlative and comparatives. Intuitively, objective records of news events require less flavor words, while flavorful headlines are more likely to contain bias. We determine the flavor words using the part-of-speech tagger from NLTK \cite{NLTK09}. Since longer sentences are more likely to contain flavor words, we define a normalized flavor metric as the number of flavor words, divided by the total number of words in a sentence.

Similar to the topic sentiment analysis, we compare the flavor per news source and topic with the average flavor taken over all headlines in one topic, the \textit{mean topic flavor}. It is important to note that a deviation from the mean topic flavor does not indicate that a news source is negatively or positively biased for one topic. Instead, such a deviation means that a news source uses significantly more subjective words than others, which we consider an indicator of bias using the definition in Section \ref{sec:intro}. 

\subsubsection{Domain Classification}\label{sec:domain classification}
We implemented a Bag of Words model with a Random Forest Classifier to detect the domain for a given headline. For this task, we handpicked six domains. Those were the five most occurring domains: BBC, CNN, Independent, Reuters, and The Guardian. We additionally selected Foxnews as a sixth domain, hoping for it to be a contrast to the other five ones. The same classification task was performed for a different selection of domains. This time, we selected 16 of the most occurring domains, but limiting the timespan to the year 2017.

The number of headlines per domain varies a lot, ranging from just 1,418 headlines (Foxnews) to over 11,000 headlines (BBC). Due to this big imbalance, we applied undersampling to create a balanced dataset, containing 8,508 total headlines for the classification for six domains and 6,096 headlines for the classification for 16 domains. Then we split the data into a training set (80\%) and a test set (20\%).

The Bag of Words model was created by firstly removing stopwords from the headlines and then transforming them into vectors with maximum 5000 features, using a CountVectorizer. Secondly, a Random Forest Classifier was used to predict the domain for the headlines. We found, by trial and error, that 50 is a good number estimators for this task.

\subsection{Evaluation}\label{sec:evaluation}

In this section, we will describe the evaluation metrics used to validate our systems. Since there is no good quantitative measure to evaluate the quality of the detected topics, we instead validated them using a small-scale study in which three participants were asked to define a topic from the top twenty most distinct keywords as found by the BTM, and give a rating on a scale from 1 to 7 for how coherent the topic was. A large-scale study was beyond the scope of this project due to time limitations. We selected a subset of the twenty topics based on average coherence rating and potential for controversy to perform sentiment analysis over. The selected topics can be found in Table \ref{tab:topics}. After extraction of these selected topics, the data set was severely reduced in size. The distribution of headlines per topic is shown in Fig. \ref{fig:topic_dist}, the number of headlines per news source after selecting topics is shown in Fig. \ref{fig:domain_dist}.

\begin{figure}[!hbt]
  
  \centering
    \includegraphics[width=\columnwidth]{topic_dist_bars.png}
    \caption{Number of headlines per topic.}
  \label{fig:topic_dist}
\end{figure}

\begin{figure}[!hbt]
  
  \centering
    \includegraphics[width=\columnwidth]{domain_dist.png}
    \caption{Number of headlines per news source.}
  \label{fig:domain_dist}
\end{figure}

We validated the per-topic sentiment and flavor analysis using a two-sided paired T-test. For every news source, the measurements were tested against the rest of the population excluding the news source. We did not assume homogeneity of variance across samples. The results of these tests will be shown in Section \ref{sec:results}.

We evaluated the classifier by taking its F1-Score into account, but also looking at other influencial factors, such as the number of classes. A confusion matrix (see Fig. \ref{fig:classificationConfusionMatrix}) helped us to detect domains which were identical to others. For example, \texttt{bbc.co.uk} was often confused with \texttt{bbc.com}. Since headlines of both of those domains come from BBC, we aggregated them into a single class. The same was done for Reuters, CNN and The Guardian. Furthermore, we eliminated domains that regularly post headlines that came from other domains. One example for that is \texttt{yahoo.com}, which takes news stories from various sources and publishes them on their domain. The classification revealed those sources accordingly.

\section{Results}\label{sec:results}

%{\bf Each team member must submit an individual report.}

%            Include relevant observations, measurements, and statistics.
%            For example, for the VLSI Class: Include statistics such as
%            timing information if available by simulation, or if not,
 %           your own analysis about critical path, delays, and clock
%            cycles. Be sure to include size information: the total size
 %           of the circuit measured (X lambda by Y lambda), and the
 %           transistor count. 

\subsection{General Sentiment Analysis}

Average sentiment values for the headlines per domain ranged from $-0.3820$ to $-0.0218$. Domains with less than 250 headlines were not included in this evaluation. A selection of the most occurring domains and their average sentiment values is shown in Table \ref{tab:averageSentiments}.

\subsection{Sentiment and Flavor Analysis}\label{sec:sentiment flavor results}

The results of the sentiment analysis described in Section \ref{sec:project}.\ref{sec:topic sent} evaluated using a paired T-test are shown in Fig. \ref{fig:mean_sent}. Fig. \ref{fig:mean_flavor} shows the results of the flavor analysis described in Section \ref{sec:project}.\ref{sec:flavor}.

\begin{figure}[!hbt]
  
  \centering
    \includegraphics[width=\columnwidth]{mean_sent.png}
    \caption{Deviation from mean topic sentiment per domain. The top row shows the mean topic sentiment in blue. Significant results with $p < 0.05$ are shown in red.}
  \label{fig:mean_sent}
\end{figure}

\begin{figure}[!hbt]
  
  \centering
    \includegraphics[width=\columnwidth]{mean_flavor.png}
    \caption{Deviation from mean topic flavor per domain. The top row shows the mean topic flavor in blue. Significant results with $p < 0.05$ are shown in red.}
  \label{fig:mean_flavor}
\end{figure}

\subsection{Domain Classification}\label{sec:domain classification results}

The domain classification for our six selected domains yielded an F1-Score of around $0.4842$. The confusion matrix for the classification is shown in Fig. \ref{fig:classificationConfusionMatrix}.

For 16 domains, the F1-Score dropped to $0.2913$. In this case the confusion matrix shown in Fig. \ref{fig:classificationConfusionMatrix2} shows bigger differences between the single domains.

Additionally, we took a look at the feature importances and we examined the words with the highest prediction probabilities for a certain domain. Table \ref{tab:indicativeWords} shows a list of some of the most indicative words.

\begin{figure}[!hbt]
  \centering
    \includegraphics[width=\columnwidth]{classificationConfusionMatrix.png}
    \caption{Confusion matrix for the domain classification of six domains.}
  \label{fig:classificationConfusionMatrix}
\end{figure}

\begin{figure}[!hbt]
  \centering
    \includegraphics[width=\columnwidth]{classificationConfusionMatrix2.png}
    \caption{Confusion matrix for the domain classification of 16 domains.}
  \label{fig:classificationConfusionMatrix2}
\end{figure}

\section{Conclusions}\label{sec:conclusions}

Detecting biases in newspapers is an important task. It helps guide readers and exposes hidden biases that are present in news headlines. There are many possible approaches to implement an automatic bias detection system using machine learning. One such approach is Sentiment Analysis, which we used as well in combination with Topic Detection. Another method that we used is measuring the amount of flavor words, i.e. adjectives and adverbs. Our third and final method is to classify news headlines into their respective sources.

Merely applying Sentiment Analysis to a single headline is not accurate enough to detect true sentiments or biases. In fact, the individual values often appear disappointingly inaccurate. However, if the number of headlines is big enough, the aggregated mean values per domain show significant differences from the general mean. This proves that there are indeed great differences between the domains, but that could also be because each domain focuses on different topics. A topic might carry negative or positive sentiment because of its nature, regardless of who talks about it. Therefore, we decided to examine sentiments per topic to find out whether different domains talk about the same topic differently.

Since big deviations are expected due to the relative inaccuracy of the Sentiment Analysis, statistical tests were performed to detect whether those deviations are significant. Looking at the results of the sentiment analysis per topic, we first note that the distribution of sentiment values is informative, since a random distribution would only yield three significant deviations on average. The mean topic sentiment values are negative for all topics except the last topic (``Israel''), which confirms that news reports often yield a negative sentiment value overall. The most interesting result is the large negative relative sentiment value for the topic ``Israel'' for \texttt{rt.com} (Russia Today).

The same statistical tests were performed for the results of the Flavor Analysis. Several significant values could be found. The most interesting fact is probably that both \texttt{nytimes.com} and \texttt{bloomberg.com} yield significantly lower values for most topics than the other domains in the comparison. This does not necessarily tell us anything about bias. However, it might be explainable by mere journalistic style, since the lack of adverbs is often regarded as a good style, and some domains might pay more attention to that than others.

The domain classification provided a different perspective on biases in news headlines. In contrast to Sentiment Analysis, it cannot tell us anything about positivity or negativity of a domain towards a specific topic, but it can detect a general bias. A higher classification score means that there are existing biases in the domains. If a domain is being classified well, it means that it has distinct features that other domains do not have. For six handpicked domains, there was an F1-Score of $0.4842$, which clearly proves a bias. For 16 domains, the F1-Score was still at $0.2913$. Here, it becomes clear that some domains can be classified better than others.

Bias might be the explanation for some of those differences. However, it is not a reliable indicator, since a high or a low accuracy for a domain can have many reasons. For example, the classifier performs very poorly for Yahoo. This can be explained by the fact that Yahoo usually doesn't publish news itself, but rather takes news stories from other news sources and publishes them on Yahoo. There are other possible explanations for a high or low accuracy which have nothing to do with bias, such as journalistic style.

To conclude, we found that there are measurable biases in newspaper headlines. Classification yielded surprisingly good results. The detected differences between the respective domains are statistically significant.

During this project we deepened our understanding of Sentiment Analysis, Natural Language Processing, Topic Detection and Text Classification. We practically applied all of those methods to obtain significant results and to detect biases in newspaper headlines. We are generally happy with the way we conducted this project.

However, there are a few things that we would have done differently. We would have definitely benefitted from a larger, more complete dataset. If in our data, a single event would have been covered by multiple news sources, we could have performed a direct comparison per event. The fact that the dataset does not include many headlines probably raised the deviations.

Therefore, we had to give up on a few methods that would have worked on a complete dataset. Research should focus on a single method generally, but in our case it was probably good to try different methods and to find out which ones work well.

There are numerous possible approaches to detecting newspaper biases, and we only scratched the surface of this broad topic. Further research is definitely going to reveal more.

The code for this research can be found at \texttt{github.com/Constantin1225561/CCMLWI-NewspaperBias}.

\subsection{Contributions}\label{sec:contributions}

The specific author contributions are as follows. C. Br\^{i}ncoveanu scraped the Reddit data set, performed general sentiment analysis per domain, created the domiain classifier and evaluated it. Additionally, this author wrote the sections 
\begin{itemize}
\item Abstract,
\item \ref{sec:project}. Project,
\item \ref{sec:project}.\ref{sec:data}. Data,
\item \ref{sec:project}.\ref{sec:general sentiment}. General Sentiment Analysis,
\item \ref{sec:project}.\ref{sec:domain classification}. Domain Classification,
\item \ref{sec:project}.\ref{sec:evaluation}. Evaluation, part on domain classification,
\item \ref{sec:results}.\ref{sec:domain classification results}. Domain Classification (Results),
\item \ref{sec:conclusions}. Conclusions.
\end{itemize}

T. van Niedek conducted part of the literature review, created the topic detection system and validation questionnaires, performed topic sentiment analysis and flavor analysis, and validated these results. The sections written by this author are 
\begin{itemize}
\item \ref{sec:intro}. Introduction,
\item \ref{sec:background}. Background,
\item \ref{sec:project}.\ref{sec:topic detection}. Topic Detection,
\item \ref{sec:project}.\ref{sec:topic sent}. Topic Sentiment Analysis,
\item \ref{sec:project}.\ref{sec:flavor}. Flavor Analysis,
\item \ref{sec:project}.\ref{sec:evaluation}. Evaluation, parts on topic detection, sentiment and flavor analyses,
\item \ref{sec:results}.\ref{sec:sentiment flavor results}. Sentiment and Flavor Analysis (Results).
\end{itemize}

The majority of the ideas for research directions that were generated were a combination of efforts from both authors.

           
\begin{thebibliography}{99}
\bibitem{LDA03} D. M. Blei, A. Y. Ng and M. I. Jordan, ``Latent Dirichlet Allocation'' {\it Journal of machine Learning research},  Jan. (2003), pp.~993--1022.

\bibitem{NLTK09} Steven Bird, Ewan Klein and Edward Loper. {\it Natural language processing with Python: analyzing text with the natural language toolkit}, ``O'Reilly Media, Inc.'', 2009.

\bibitem{Caverni90} Caverni J.-P., Fabre J. M., Gonzales, M. (1990): Cognitive Biases: Their Contribution for Understanding Human Cognitive Processes. Amsterdam: {\it Elsevier Science Publishers B. V.}

\bibitem{CHU} Chu, Albert, Kensen Shi, and Catherine Wong, ``Prediction of Average and Perceived Polarity in Online Journalism.''

\bibitem{DOUMIT11} Doumit, Sarjoun, and Ali Minai, ``Online news media bias analysis using an LDA-NLP approach'', {\it International Conference on Complex Systems}, 2011.

\bibitem{FST13} Thomas L. Griffiths and Mark Steyvers, ``Finding scientific topics'', {\it Proceedings of the National academy of Sciences}, vol.~101, suppl 1, 2004 pages 5228--5235.

\bibitem{GUPTA09} Gupta, Sonal. ``Finding bias in political news and blog websites'', 2009.

\bibitem{VAD14} Clayton J. Hutto and Eric Gilbert, 
   ``Vader: A parsimonious rule-based model for sentiment analysis of social media text.'', {\it  Eighth international AAAI conference on weblogs and social media}, Ann Arbor, MI, USA, June 2014.

\bibitem{Lichman13} Lichman, M. (2013). {\it UCI Machine Learning Repository} \texttt{http://archive.ics.uci.edu/ml}. Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{BTM15} Joan Capdevila Pujol (Universitat Polyt\`{e}cnica de Catalunya), ``jcapde/Biterm: Biterm topic model'', {\it GitHub}, \texttt{https://github.com/jcapde/Biterm}, posted October 14, 2015, accessed May 24, 2017.

\bibitem{RECASENS13} Recasens, Marta, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky, ``Linguistic Models for Analyzing and Detecting Biased Language'', {\it ACL (1).} 2013.

\bibitem{STOCKING89} S. Holly Stocking and Paget H. Gross, {\it How Do Journalists Think? A proposal for the study of cognitive bias in newsmaking}, ERIC Clearinghouse on Reading and Communication Skills, 2805 E. 10th St., Smith Research Center, Suite 150, Bloomington, IN 47405, 1989.

\bibitem{BTM13} Xiaohui Yan, Jiafeng Guo, Yanyan Lan and Xueqi Cheng, 
   ``A Biterm Topic Model for Short Texts'', {\it Proceedings of the 22nd international conference on 
   World Wide Web}, ACM, 2013, pages 1445--1456.


   
\end{thebibliography}

\newpage

\hbox{}


\newpage

\thispagestyle{empty}

\onecolumn

\section*{Appendix}

\begin{table}[htb]
\caption{A selection of the most occurring domains and their average sentiment values.}
\begin{tabularx}{\textwidth}{X|l|l}
Domain & Number of headlines & Average Sentiment \\ \hline
bloomberg.com & 1738 & -0.1231787112 \\ \hline
haaretz.com & 2106 & -0.1742229345 \\ \hline
cbc.ca & 2041 & -0.1900515924 \\ \hline
rt.com & 2633 & -0.1984226358 \\ \hline
spiegel.de & 1839 & -0.1984747145 \\ \hline
news.bbc.co.uk & 5258 & -0.2008100609 \\ \hline
t.co & 24799 & -0.2058326384 \\ \hline
atimes.com & 2893 & -0.2079524369 \\ \hline
nytimes.com & 7225 & -0.2083552803 \\ \hline
reuters.com & 11572 & -0.2086531455 \\ \hline
reut.rs & 11593 & -0.2091130337 \\ \hline
washingtonpost.com & 3209 & -0.2096432845 \\ \hline
jpost.com & 1812 & -0.212358223 \\ \hline
abc.net.au & 1933 & -0.2137741335 \\ \hline
news.com & 9438 & -0.2157797203 \\ \hline
google.com & 2403 & -0.2189052434 \\ \hline
theguardian.com & 7427 & -0.2213403797 \\ \hline
bbc.co.uk & 14817 & -0.2224216913 \\ \hline
yahoo.com & 5464 & -0.224336347 \\ \hline
bbc.com & 4362 & -0.2269494956 \\ \hline
telegraph.co.uk & 4733 & -0.2331175364 \\ \hline
news.yahoo.com & 4710 & -0.2333904883 \\ \hline
globalpost.com & 2178 & -0.2352555096 \\ \hline
cnn.com & 5537 & -0.2354793209 \\ \hline
guardian.co.uk & 7591 & -0.2390904887 \\ \hline
on.cnn.com & 1801 & -0.2417565797 \\ \hline
edition.cnn.com & 1799 & -0.24187204 \\ \hline
independent.co.uk & 5367 & -0.2497368176 \\ \hline
aljazeera.net & 2115 & -0.2740432624 \\ \hline
dailymail.co.uk & 1964 & -0.2797059063 \\ \hline
english.aljazeera.net & 2026 & -0.2802155972 \\ \hline
aljazeera.com & 2888 & -0.2809025277 \\
\end{tabularx}
\label{tab:averageSentiments}
\end{table}

\begin{table}[htb]
\caption{Selected topics within the 2017 Reddit data set}
\begin{tabularx}{\textwidth}{X|l|l}
Keywords & Topic & Average Coherence \\ \hline
election, EU, Brexit, vote, party, president, minister, UK, parliament, presidential, Theresa, May, says, prime, European, Le, Pen, government, would, French & EU Politics & $5.67$ \\ \hline
Russian, Russia, US, intelligence, Trump, hacking, hackers, attack, cyber, CIA, FBI, security, government, former, ex, spy, data, says, officials & Cyber security \& Russian hacking & $6.00$ \\ \hline
Germany, party, Trump, right, says, German, Nazi, world, anti, social, pope, new, election, far, Brexit, leader, president, French, speech, saying & Right-wing politics \& populism & $4.33$ \\ \hline
Syria, Russia, US, north, Korea, Trump, military, missile, UN, war, Israel, turkey, Iran, state, said, Syrian, says, united, forces & Middle East conflicts & $5$ \\ \hline
Israel, Trump, Israeli, president, Netanyahu, house, white, US, Palestinian, Donald, says, minister, Jerusalem, bank, peace, Palestinians, visit, settlements, west, two & Israel & $5.66$ \\
\end{tabularx}
\label{tab:topics}
\end{table}

\begin{table}[htb]
\caption{Most indicative words for the domain classification}
\begin{tabularx}{\textwidth}{X|l|l}
Word & Highest probability & Predicted domain \\ \hline
mosul & 0.94 & CNN \\ \hline
us & 0.84 & CNN \\ \hline
sues & 0.815 & BBC \\ \hline
aleppo & 0.8 & CNN \\ \hline
reportedly & 0.8 & Foxnews \\ \hline
japan & 0.78 & CNN \\ \hline
turns & 0.77 & BBC \\ \hline
investigates & 0.76 & BBC \\ \hline
cnn & 0.76 & CNN \\ \hline
pepe & 0.747 & BBC \\ \hline
cyclone & 0.745 & BBC \\ \hline
bbc & 0.727 & BBC \\ \hline
erdo & 0.725 & The Guardian \\ \hline
billion & 0.72 & Reuters \\ \hline
indonesian & 0.72 & Foxnews \\ \hline
football & 0.718 & BBC \\
\end{tabularx}
\label{tab:indicativeWords}
\end{table}

\end{document}
