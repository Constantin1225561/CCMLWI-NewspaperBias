{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# random\n",
    "from random import shuffle\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"data/reddit2017.csv\", header=0)\n",
    "un, counts = np.unique(df[\"Domain\"].as_matrix(), return_counts=True)\n",
    "#print(un)\n",
    "#print(counts)\n",
    "\n",
    "selected_domains = []\n",
    "for i in range(0, len(un)):\n",
    "    if counts[i] > 350:\n",
    "        selected_domains.append(un[i])\n",
    "selected_domains = [\"foxnews.com\", \"aljazeera.com\", \"bbc.co.uk\",\n",
    "                    \"bloomberg.com\",\n",
    "                   \"reuters.com\"]\n",
    "\n",
    "selected_domains = ['abcnews.go.com','aljazeera.com','bbc.com','bloomberg.com',\n",
    " 'cbc.ca','cnn.com','dw.com','foxnews.com',\n",
    " 'independent.co.uk','nytimes.com','reuters.com','rt.com','uk.reuters.com','washingtonpost.com']\n",
    "\n",
    "df = df[df[\"Domain\"].isin(selected_domains)]\n",
    "un, counts = np.unique(df[\"Domain\"].as_matrix(), return_counts=True)\n",
    "print(un)\n",
    "print(counts)\n",
    "min_counts = min(counts)\n",
    "#print(min_counts)\n",
    "\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(selected_domains)):\n",
    "    domain = selected_domains[i]\n",
    "    ddf = df[df[\"Domain\"] == domain]\n",
    "    ddf = ddf.head(min_counts)\n",
    "    msk = np.random.rand(len(ddf)) < 0.8\n",
    "    train = train.append(ddf[msk])\n",
    "    test = test.append(ddf[~msk])\n",
    "\n",
    "train.to_csv( \"data/train.csv\", index=False, quotechar='\"',escapechar='\\\\')\n",
    "test.to_csv( \"data/test.csv\", index=False, quotechar='\"',escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"data/train.csv\", header=0, encoding='latin1')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#print (stopwords.words(\"english\") )\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def title_to_words( raw_title ):\n",
    "    title_text = BeautifulSoup(raw_title).get_text()     \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", title_text)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return( \" \".join( meaningful_words ))\n",
    "num_titles = train[\"Title\"].size\n",
    "clean_train_titles = []\n",
    "for i in range( 0, num_titles ):\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print (\"Title %d of %d\" % ( i+1, num_titles ))\n",
    "    clean_train_titles.append( title_to_words( train[\"Title\"][i] ) )\n",
    "#print (clean_train_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://linanqiu.github.io/2015/10/07/word2vec-sentiment/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
